"""\nIndiaMART Web Scraper\nExtracts product and supplier data from IndiaMART B2B marketplace\n"""\nimport requests\nfrom bs4 import BeautifulSoup\nimport logging\nfrom typing import List, Dict\nimport re\nfrom .utils import RequestHelper, save_json, save_csv\nfrom .config import ScraperConfig\n\nlogger = logging.getLogger(__name__)\n\nclass IndiaMartScraper:\n    def __init__(self):\n        self.base_url = "https://www.indiamart.com"\n        self.helper = RequestHelper()\n        self.config = ScraperConfig()\n        self.session = requests.Session()\n        \n    def search_category(self, category: str, max_results: int = 100) -> List[Dict]:\n        """\n        Search for products in a specific category\n        \n        Args:\n            category: Product category to search\n            max_results: Maximum number of products to scrape\n            \n        Returns:\n            List of product dictionaries\n        """\n        products = []\n        search_url = f"{self.base_url}/impcat/{category}.html"\n        \n        try:\n            page = 1\n            while len(products) < max_results:\n                logger.info(f"Scraping {category} - Page {page}")\n                \n                # Add pagination\n                url = f"{search_url}?page={page}" if page > 1 else search_url\n                \n                response = self.session.get(\n                    url, \n                    headers=self.helper.get_headers(),\n                    timeout=30\n                )\n                \n                if response.status_code != 200:\n                    logger.warning(f"Failed to fetch page {page}: Status {response.status_code}")\n                    break\n                \n                soup = BeautifulSoup(response.content, 'lxml')\n                product_cards = soup.find_all('div', class_=re.compile('lst|card|product'))\n                \n                if not product_cards:\n                    logger.info(f"No more products found for {category}")\n                    break\n                \n                for card in product_cards:\n                    if len(products) >= max_results:\n                        break\n                        \n                    product_data = self._extract_product_data(card, category)\n                    if product_data:\n                        products.append(product_data)\n                \n                # Random delay to avoid rate limiting\n                self.helper.random_delay(*self.config.delay_range)\n                page += 1\n                \n                # Safety: max 10 pages\n                if page > 10:\n                    break\n                    \n        except Exception as e:\n            logger.error(f"Error scraping category {category}: {str(e)}")\n        \n        return products\n    \n    def _extract_product_data(self, card, category: str) -> Dict:\n        """Extract product information from HTML card"""\n        try:\n            product = {\n                'category': category,\n                'product_name': '',\n                'price': '',\n                'price_unit': '',\n                'supplier_name': '',\n                'supplier_location': '',\n                'description': '',\n                'url': '',\n                'image_url': ''\n            }\n            \n            # Extract product name\n            name_elem = card.find(['h2', 'h3', 'a'], class_=re.compile('title|name|pname'))\n            if name_elem:\n                product['product_name'] = name_elem.get_text(strip=True)\n                if name_elem.get('href'):\n                    product['url'] = self.base_url + name_elem['href'] if not name_elem['href'].startswith('http') else name_elem['href']\n            \n            # Extract price\n            price_elem = card.find(['span', 'div'], class_=re.compile('price|rs'))\n            if price_elem:\n                price_text = price_elem.get_text(strip=True)\n                product['price'] = price_text\n                # Extract unit if present\n                unit_match = re.search(r'/(\\w+)', price_text)\n                if unit_match:\n                    product['price_unit'] = unit_match.group(1)\n            \n            # Extract supplier info\n            supplier_elem = card.find(['span', 'div', 'a'], class_=re.compile('company|supplier|seller'))\n            if supplier_elem:\n                product['supplier_name'] = supplier_elem.get_text(strip=True)\n            \n            # Extract location\n            location_elem = card.find(['span', 'div'], class_=re.compile('location|city|address'))\n            if location_elem:\n                product['supplier_location'] = location_elem.get_text(strip=True)\n            \n            # Extract description\n            desc_elem = card.find(['p', 'div'], class_=re.compile('desc|detail'))\n            if desc_elem:\n                product['description'] = desc_elem.get_text(strip=True)[:500]\n            \n            # Extract image\n            img_elem = card.find('img')\n            if img_elem and img_elem.get('src'):\n                product['image_url'] = img_elem['src']\n            \n            # Only return if we have at least product name\n            if product['product_name']:\n                return product\n                \n        except Exception as e:\n            logger.error(f"Error extracting product data: {str(e)}")\n        \n        return None\n    \n    def scrape_all_categories(self) -> List[Dict]:\n        """Scrape all configured categories"""\n        all_products = []\n        \n        for category in self.config.categories:\n            logger.info(f"Starting to scrape category: {category}")\n            products = self.search_category(category, self.config.max_products)\n            all_products.extend(products)\n            logger.info(f"Scraped {len(products)} products from {category}")\n        \n        logger.info(f"Total products scraped: {len(all_products)}")\n        return all_products\n    \n    def save_data(self, data: List[Dict], output_format='both'):\n        """Save scraped data to file"""\n        if output_format in ['json', 'both']:\n            save_json(data, 'data/raw/indiamart_products.json')\n        \n        if output_format in ['csv', 'both']:\n            save_csv(data, 'data/raw/indiamart_products.csv')\n