"""\nUtility functions for web scraping\n"""\nimport time\nimport random\nimport logging\nfrom fake_useragent import UserAgent\n\n# Setup logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\nlogger = logging.getLogger(__name__)\n\nclass RequestHelper:\n    def __init__(self):\n        self.ua = UserAgent()\n        \n    def get_headers(self):\n        """Generate random headers to avoid detection"""\n        return {\n            'User-Agent': self.ua.random,\n            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n            'Accept-Language': 'en-US,en;q=0.5',\n            'Accept-Encoding': 'gzip, deflate',\n            'Connection': 'keep-alive',\n            'Upgrade-Insecure-Requests': '1'\n        }\n    \n    def random_delay(self, min_delay=2, max_delay=5):\n        """Add random delay between requests"""\n        delay = random.uniform(min_delay, max_delay)\n        logger.info(f"Sleeping for {delay:.2f} seconds...")\n        time.sleep(delay)\n    \n    def exponential_backoff(self, attempt, base_delay=2):\n        """Implement exponential backoff for retries"""\n        delay = base_delay * (2 ** attempt) + random.uniform(0, 1)\n        logger.warning(f"Retry attempt {attempt}, waiting {delay:.2f} seconds")\n        time.sleep(delay)\n\ndef save_json(data, filename):\n    """Save data to JSON file"""\n    import json\n    with open(filename, 'w', encoding='utf-8') as f:\n        json.dump(data, f, indent=2, ensure_ascii=False)\n    logger.info(f"Data saved to {filename}")\n\ndef save_csv(data, filename):\n    """Save data to CSV file"""\n    import pandas as pd\n    df = pd.DataFrame(data)\n    df.to_csv(filename, index=False, encoding='utf-8')\n    logger.info(f"Data saved to {filename}")\n